sampled_points <- matrix(0, nrow = N, ncol = d)
for (i in seq_len(d)) {
cell_centers <- grid_coords[sampled_indices, i]
offsets <- runif(N, min = -grid_step[i] / 2, max = grid_step[i] / 2)  # Random offset within cell
sampled_points[, i] <- cell_centers + offsets
}
# Return all points and rates
list(
sampled_points = sampled_points,  # Points sampled from the LGCP
rates = Z,                        # GRF rates
grid_coords = X                   # Grid coordinates
)
}
result <- simulate_lgcp(
d = 3,
bounds = list(c(0, 10), c(0, 10), c(0, 10)),
m = 10,
length_scale = 2,
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] },
covariate_coeff = 0.05,
seed = 123
)
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] }
sampled_points <- result$sampled_points
tetra <- delaunayn(sampled_points)
library(geometry)
tetra <- delaunayn(sampled_points)
mesh <- fm_mesh_3d(sampled_points,tetra)
Q <- fm_matern_precision(mesh, alpha = 2, rho=1, sigma=1)
Q_mat <- as.matrix(Q)
Q_sym <- (Q_mat+t(Q_mat))/2
Q_sym <- Q_sym + diag(1e-6, nrow(Q_sym))
library(rstan)
covariate_effect <- apply(sampled_points,1,covariate_field)
# Define Stan data
stan_data <- list(
M = nrow(Q_sym),         # Number of grid points (should be 1000)
Q = Q_mat,             # Your computed precision matrix
covariate_effect = covariate_effect,  # Covariate effects
volume = 10^3,                    # Total volume of the 3D space
counts = rep(0, nrow(Q_sym))  # Placeholder for observed counts
)
# Compile and fit the model
fit <- stan(file = "~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan", data = stan_data, iter = 2000, chains = 4)
# Define Stan data
stan_data <- list(
M = nrow(Q_sym),         # Number of grid points (should be 1000)
Q = Q_sym,             # Your computed precision matrix
covariate_effect = covariate_effect,  # Covariate effects
volume = 10^3,                    # Total volume of the 3D space
counts = rep(0, nrow(Q_sym))  # Placeholder for observed counts
)
# Compile and fit the model
fit <- stan(file = "~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan", data = stan_data, iter = 2000, chains = 4)
print(eigen(Q_sym)$values)
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
# Define Stan data
stan_data <- list(
M = nrow(Q_sym),         # Number of grid points (should be 1000)
Q = Q_sym,             # Your computed precision matrix
covariate_effect = covariate_effect,  # Covariate effects
volume = 10^3,                    # Total volume of the 3D space
counts = rep(0, nrow(Q_sym))  # Placeholder for observed counts
)
# Compile and fit the model
fit <- stan(file = "~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan", data = stan_data, iter = 2000, chains = 4)
library(INLA)
library(Matrix)
library(fmesher)  # Required for FEM calculations
library(rgl)  # For visualization
library(geometry)
library(rstan)
library(FNN)
library(ggplot2)
install.packages("fmesher")
simulate_lgcp <- function(d, bounds, m, length_scale, covariate_field, covariate_coeff, seed = 123) {
set.seed(seed)
xlims <- bounds  # List of bounds for each dimension
grid_sides <- lapply(xlims, function(lim) seq(lim[1], lim[2], length.out = m)) # Calculates the size of each grid cell along each dimension
# Create grid coordinates
grid_coords <- expand.grid(grid_sides)
X <- as.matrix(grid_coords)
grid_step <- sapply(xlims, function(lim) diff(lim) / m)  # Size of each grid cell
# Powered Exponential Covariance function
rbf <- function(X1, X2, length_scale) {
dists <- as.matrix(dist(rbind(X1, X2)))
exp(-dists[1:nrow(X1), (nrow(X1)+1):(nrow(X1)+nrow(X2))] ^ 2 / (2 * length_scale ^ 2))
}
# Draw sample from Gaussian Process
K <- rbf(X, X, length_scale)
Y <- MASS::mvrnorm(mu = rep(0, nrow(X)), Sigma = K) #Draws samples from a multivariate normal distribution
# Y represents the Gaussian Random Field
# Covariate effect: apply covariate field
covariate_values <- apply(X, 1, covariate_field)
covariate_effect <- exp(covariate_coeff * covariate_values)
# Incorporate covariates
Z <- exp(Y) * covariate_effect  # Modify the rate with the covariate effect
# Generate points proportional to GRF rates
volume <- prod(sapply(xlims, diff))  # Volume of the d-dimensional space
total_rate <- sum(Z) * (volume / (m^d))  # Integral approximation over grid
N <- rpois(1, total_rate)  # Total number of points
# Sample grid cells proportional to GRF rates
probabilities <- Z / sum(Z)  # Normalize rates to probabilities
sampled_indices <- sample(1:length(Z), size = N, replace = TRUE, prob = probabilities)
# Generate random points within the sampled grid cells
sampled_points <- matrix(0, nrow = N, ncol = d)
for (i in seq_len(d)) {
cell_centers <- grid_coords[sampled_indices, i]
offsets <- runif(N, min = -grid_step[i] / 2, max = grid_step[i] / 2)  # Random offset within cell
sampled_points[, i] <- cell_centers + offsets
}
# Return all points and rates
list(
sampled_points = sampled_points,  # Points sampled from the LGCP
rates = Z,                        # GRF rates
grid_coords = X                   # Grid coordinates
)
}
result <- simulate_lgcp(
d = 3,
bounds = list(c(0, 10), c(0, 10), c(0, 10)),
m = 10,
length_scale = 2,
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] },
covariate_coeff = 0.05,
seed = 123
)
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] }
sampled_points <- result$sampled_points
simplices <- delaunayn(grid_coords)  # Each row is a simplex (e.g. tetrahedron in 3D)
library(geometry)
simplices <- delaunayn(grid_coords)  # Each row is a simplex (e.g. tetrahedron in 3D)
simplices <- delaunayn(sampled_points)  # Each row is a simplex (e.g. tetrahedron in 3D)
compute_fem_matrices <- function(points, simplices) {
library(Matrix)
n_points <- nrow(points)
d <- ncol(points)
n_simplices <- nrow(simplices)
C <- Matrix(0, nrow = n_points, ncol = n_points, sparse = TRUE)
G <- Matrix(0, nrow = n_points, ncol = n_points, sparse = TRUE)
for (i in 1:n_simplices) {
idx <- simplices[i, ]
verts <- points[idx, ]  # shape (d+1, d)
# Compute volume of the simplex
V <- abs(det(cbind(verts[-1, ] - matrix(verts[1, ], nrow = d, ncol = d, byrow = TRUE)))) / factorial(d)
# Build local stiffness matrix
D <- cbind(1, verts)  # (d+1) x (d+1)
grads <- solve(D)[-1, ]  # (d) x (d+1), gradients of basis functions
G_local <- matrix(0, d + 1, d + 1)
for (a in 1:(d + 1)) {
for (b in 1:(d + 1)) {
G_local[a, b] <- V * sum(grads[, a] * grads[, b])
}
}
# Local mass matrix (linear FEM, lumped version)
C_local <- matrix(V / ((d + 1)*(d + 2)), d + 1, d + 1)
diag(C_local) <- 2 * V / ((d + 1)*(d + 2))
# Assemble into global matrices
for (a in 1:(d + 1)) {
for (b in 1:(d + 1)) {
C[idx[a], idx[b]] <- C[idx[a], idx[b]] + C_local[a, b]
G[idx[a], idx[b]] <- G[idx[a], idx[b]] + G_local[a, b]
}
}
}
list(C = C, G = G)
}
fem <- compute_fem_matrices(sampled_points, simplices)
C <- fem$C
G <- fem$G
# Build SPDE precision matrix
kappa <- 1  # Adjust to control range
Q <- kappa^2 * C + G
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
a
install.packages("cmdstanr")
cmdstanr::install_cmdstan(version = "2.35.0")  # Or latest available
library(cmdstanr)
model <- cmdstan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # This compiles your model
model <- cmdstan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # This compiles your model
model <- cmdstan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # This compiles your model
install_cmdstan(version = "2.35.0")
cmdstanr::cmdstan_version()
model <- cmdstan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # This compiles your model
stan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # <- from the `rstan` package
stan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # <- from the `rstan` package
library(cmdstanr)
stan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # <- from the `rstan` package
model <- cmdstan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")  # This compiles your model
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
rstan:::rstudio_stanc("Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
# Assume: you have C and Q as sparse matrices, sampled_points, and simplices
obs_indices <- FNN::get.knnx(sampled_points, sampled_points, k = 1)$nn.index  # if mesh = data
stan_data <- list(
N_nodes = nrow(sampled_points),
N_obs = nrow(sampled_points),
obs_idx = as.vector(obs_indices),
C = as.matrix(C),  # or sparse format for CmdStanR
Q = as.matrix(Q)
)
mod <- cmdstan_model("~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan")
fit <- stan(
file = "~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan",
data = stan_data,
chains = 4,
iter = 2000,
warmup = 1000,
seed = 123,
cores = 4,
control = list(adapt_delta = 0.95)
)
library(rstan)
fit <- stan(
file = "~/Documents/Skole/H2024/master/code/Point_Processes/try_inference.stan",
data = stan_data,
chains = 4,
iter = 2000,
warmup = 1000,
seed = 123,
cores = 4,
control = list(adapt_delta = 0.95)
)
simulate_lgcp <- function(d, bounds, m, length_scale, covariate_field, covariate_coeff, seed = 123) {
set.seed(seed)
xlims <- bounds  # List of bounds for each dimension
grid_sides <- lapply(xlims, function(lim) seq(lim[1], lim[2], length.out = m)) # Calculates the size of each grid cell along each dimension
# Create grid coordinates
grid_coords <- expand.grid(grid_sides)
X <- as.matrix(grid_coords)
grid_step <- sapply(xlims, function(lim) diff(lim) / m)  # Size of each grid cell
# Powered Exponential Covariance function
rbf <- function(X1, X2, length_scale) {
dists <- as.matrix(dist(rbind(X1, X2)))
exp(-dists[1:nrow(X1), (nrow(X1)+1):(nrow(X1)+nrow(X2))] ^ 2 / (2 * length_scale ^ 2))
}
# Draw sample from Gaussian Process
K <- rbf(X, X, length_scale)
Y <- MASS::mvrnorm(mu = rep(0, nrow(X)), Sigma = K) #Draws samples from a multivariate normal distribution
# Y represents the Gaussian Random Field
# Covariate effect: apply covariate field
covariate_values <- apply(X, 1, covariate_field)
covariate_effect <- exp(covariate_coeff * covariate_values)
# Incorporate covariates
Z <- exp(Y) * covariate_effect  # Modify the rate with the covariate effect
# Generate points proportional to GRF rates
volume <- prod(sapply(xlims, diff))  # Volume of the d-dimensional space
total_rate <- sum(Z) * (volume / (m^d))  # Integral approximation over grid
N <- rpois(1, total_rate)  # Total number of points
# Sample grid cells proportional to GRF rates
probabilities <- Z / sum(Z)  # Normalize rates to probabilities
sampled_indices <- sample(1:length(Z), size = N, replace = TRUE, prob = probabilities)
# Generate random points within the sampled grid cells
sampled_points <- matrix(0, nrow = N, ncol = d)
for (i in seq_len(d)) {
cell_centers <- grid_coords[sampled_indices, i]
offsets <- runif(N, min = -grid_step[i] / 2, max = grid_step[i] / 2)  # Random offset within cell
sampled_points[, i] <- cell_centers + offsets
}
# Return all points and rates
list(
sampled_points = sampled_points,  # Points sampled from the LGCP
rates = Z,                        # GRF rates
grid_coords = X                   # Grid coordinates
)
}
result <- simulate_lgcp(
d = 3,
bounds = list(c(0, 10), c(0, 10), c(0, 10)),
m = 10,
length_scale = 2,
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] },
covariate_coeff = 0.05,
seed = 123
)
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] }
sampled_points <- result$sampled_points
library(geometry)
simplices <- delaunayn(sampled_points)  # Each row is a simplex (e.g. tetrahedron in 3D)
mesh <- list(
loc = sampled_points,
graph = list(),
tv = simplices
)
View(mesh)
compute_fem_matrices <- function(points, simplices) {
library(Matrix)
n_points <- nrow(points)
d <- ncol(points)
n_simplices <- nrow(simplices)
C <- Matrix(0, nrow = n_points, ncol = n_points, sparse = TRUE)
G <- Matrix(0, nrow = n_points, ncol = n_points, sparse = TRUE)
for (i in 1:n_simplices) {
idx <- simplices[i, ]
verts <- points[idx, ]  # shape (d+1, d)
# Compute volume of the simplex
V <- abs(det(cbind(verts[-1, ] - matrix(verts[1, ], nrow = d, ncol = d, byrow = TRUE)))) / factorial(d)
# Build local stiffness matrix
D <- cbind(1, verts)  # (d+1) x (d+1)
grads <- solve(D)[-1, ]  # (d) x (d+1), gradients of basis functions
G_local <- matrix(0, d + 1, d + 1)
for (a in 1:(d + 1)) {
for (b in 1:(d + 1)) {
G_local[a, b] <- V * sum(grads[, a] * grads[, b])
}
}
# Local mass matrix (linear FEM, lumped version)
C_local <- matrix(V / ((d + 1)*(d + 2)), d + 1, d + 1)
diag(C_local) <- 2 * V / ((d + 1)*(d + 2))
# Assemble into global matrices
for (a in 1:(d + 1)) {
for (b in 1:(d + 1)) {
C[idx[a], idx[b]] <- C[idx[a], idx[b]] + C_local[a, b]
G[idx[a], idx[b]] <- G[idx[a], idx[b]] + G_local[a, b]
}
}
}
list(C = C, G = G)
}
fem <- compute_fem_matrices(sampled_points, simplices)
C <- fem$C
G <- fem$G
# Build SPDE precision matrix
kappa <- 1  # Adjust to control range
Q <- kappa^2 * C + G
library(INLA)
n_nodes <- nrow(sampled_points)
A <- Diagonal(n_nodes)  # Projection matrix (identity, since mesh nodes = observations)
# Fake response: counts per point (e.g., all 1s if presence-only data)
y <- rep(1, n_nodes)
stack <- inla.stack(
data = list(y = y),
A = list(A),
effects = list(z = 1:n_nodes)
)
result <- inla(
y ~ -1 + f(z, model = "z", precision = Q),
family = "poisson",
data = inla.stack.data(stack),
control.predictor = list(A = inla.stack.A(stack), compute = TRUE)
)
result <- inla(
y ~ -1 + f(z, model = "generic0", Cmatrix = Q),
family = "poisson",
data = inla.stack.data(stack),
control.predictor = list(A = inla.stack.A(stack), compute = TRUE)
)
posterior_mean <- result$summary.random$z$mean
posterior_sd <- result$summary.random$z$sd
lambda_hat <- exp(posterior_mean)  # estimated intensity
result$summary.random$z
z_summary <- result$summary.random$z
head(z_summary)
Z_true <- result$rates         # Actually this is lambda = exp(Y) * covariate
grid_coords <- result$grid_coords
rmse <- sqrt(mean((estimated_Z_grid - log_lambda_true)^2))
write.csv(result$sampled_points, file = "sampled_points.csv", row.names = FALSE)
saveRDS(result$sampled_points, file = "sampled_points.rds")
setwd("~/Documents/Skole/H2024/master/code/Point_Processes")
write.csv(result$sampled_points, file = "sampled_points.csv", row.names = FALSE)
View(sampled_points)
saveRDS(result$sampled_points, file = "sampled_points.rds")
write.table(result$sampled_points,
file = "sampled_points.txt",
row.names = FALSE,
col.names = FALSE,
sep = " ")
sampled_points <- result$sampled_points
write.table(result$sampled_points,
file = "sampled_points.txt",
row.names = FALSE,
col.names = FALSE,
sep = " ")
simulate_lgcp <- function(d, bounds, m, length_scale, covariate_field, covariate_coeff, seed = 123) {
set.seed(seed)
xlims <- bounds  # List of bounds for each dimension
grid_sides <- lapply(xlims, function(lim) seq(lim[1], lim[2], length.out = m)) # Calculates the size of each grid cell along each dimension
# Create grid coordinates
grid_coords <- expand.grid(grid_sides)
X <- as.matrix(grid_coords)
grid_step <- sapply(xlims, function(lim) diff(lim) / m)  # Size of each grid cell
# Powered Exponential Covariance function
rbf <- function(X1, X2, length_scale) {
dists <- as.matrix(dist(rbind(X1, X2)))
exp(-dists[1:nrow(X1), (nrow(X1)+1):(nrow(X1)+nrow(X2))] ^ 2 / (2 * length_scale ^ 2))
}
# Draw sample from Gaussian Process
K <- rbf(X, X, length_scale)
Y <- MASS::mvrnorm(mu = rep(0, nrow(X)), Sigma = K) #Draws samples from a multivariate normal distribution
# Y represents the Gaussian Random Field
# Covariate effect: apply covariate field
covariate_values <- apply(X, 1, covariate_field)
covariate_effect <- exp(covariate_coeff * covariate_values)
# Incorporate covariates
Z <- exp(Y) * covariate_effect  # Modify the rate with the covariate effect
# Generate points proportional to GRF rates
volume <- prod(sapply(xlims, diff))  # Volume of the d-dimensional space
total_rate <- sum(Z) * (volume / (m^d))  # Integral approximation over grid
N <- rpois(1, total_rate)  # Total number of points
# Sample grid cells proportional to GRF rates
probabilities <- Z / sum(Z)  # Normalize rates to probabilities
sampled_indices <- sample(1:length(Z), size = N, replace = TRUE, prob = probabilities)
# Generate random points within the sampled grid cells
sampled_points <- matrix(0, nrow = N, ncol = d)
for (i in seq_len(d)) {
cell_centers <- grid_coords[sampled_indices, i]
offsets <- runif(N, min = -grid_step[i] / 2, max = grid_step[i] / 2)  # Random offset within cell
sampled_points[, i] <- cell_centers + offsets
}
# Return all points and rates
list(
sampled_points = sampled_points,  # Points sampled from the LGCP
rates = Z,                        # GRF rates
grid_coords = X                   # Grid coordinates
)
}
result <- simulate_lgcp(
d = 3,
bounds = list(c(0, 10), c(0, 10), c(0, 10)),
m = 10,
length_scale = 2,
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] },
covariate_coeff = 0.05,
seed = 123
)
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] }
sampled_points <- result$sampled_points
write.table(sampled_points,
file = "sampled_points.txt",
row.names = FALSE,
col.names = FALSE,
sep = " ")
write.csv(result$sampled_points, file = "sampled_points.csv", row.names = FALSE)
write.csv(sampled_points, file = "sampled_points.csv", row.names = FALSE)
nodes <- as.matrix(read.table("/Users/Synne/Documents/Skole/H2024/master/code/Point_Processes/mesh_nodes.txt
"))
setwd("~/Documents/Skole/H2024/master/code/Point_Processes")
nodes <- as.matrix(read.table("/Users/Synne/Documents/Skole/H2024/master/code/Point_Processes/mesh_nodes.txt
"))
nodes <- as.matrix(read.table("/Users/Synne/Documents/Skole/H2024/master/code/Point_Processes/mesh_nodes.txt
"))
simulate_lgcp <- function(d, bounds, m, length_scale, covariate_field, covariate_coeff, seed = 123) {
set.seed(seed)
xlims <- bounds  # List of bounds for each dimension
grid_sides <- lapply(xlims, function(lim) seq(lim[1], lim[2], length.out = m)) # Calculates the size of each grid cell along each dimension
# Create grid coordinates
grid_coords <- expand.grid(grid_sides)
X <- as.matrix(grid_coords)
grid_step <- sapply(xlims, function(lim) diff(lim) / m)  # Size of each grid cell
# Powered Exponential Covariance function
rbf <- function(X1, X2, length_scale) {
dists <- as.matrix(dist(rbind(X1, X2)))
exp(-dists[1:nrow(X1), (nrow(X1)+1):(nrow(X1)+nrow(X2))] ^ 2 / (2 * length_scale ^ 2))
}
# Draw sample from Gaussian Process
K <- rbf(X, X, length_scale)
Y <- MASS::mvrnorm(mu = rep(0, nrow(X)), Sigma = K) #Draws samples from a multivariate normal distribution
# Y represents the Gaussian Random Field
# Covariate effect: apply covariate field
covariate_values <- apply(X, 1, covariate_field)
covariate_effect <- exp(covariate_coeff * covariate_values)
# Incorporate covariates
Z <- exp(Y) * covariate_effect  # Modify the rate with the covariate effect
# Generate points proportional to GRF rates
volume <- prod(sapply(xlims, diff))  # Volume of the d-dimensional space
total_rate <- sum(Z) * (volume / (m^d))  # Integral approximation over grid
N <- rpois(1, total_rate)  # Total number of points
# Sample grid cells proportional to GRF rates
probabilities <- Z / sum(Z)  # Normalize rates to probabilities
sampled_indices <- sample(1:length(Z), size = N, replace = TRUE, prob = probabilities)
# Generate random points within the sampled grid cells
sampled_points <- matrix(0, nrow = N, ncol = d)
for (i in seq_len(d)) {
cell_centers <- grid_coords[sampled_indices, i]
offsets <- runif(N, min = -grid_step[i] / 2, max = grid_step[i] / 2)  # Random offset within cell
sampled_points[, i] <- cell_centers + offsets
}
# Return all points and rates
list(
sampled_points = sampled_points,  # Points sampled from the LGCP
rates = Z,                        # GRF rates
grid_coords = X                   # Grid coordinates
)
}
result <- simulate_lgcp(
d = 3,
bounds = list(c(0, 10), c(0, 10), c(0, 10)),
m = 10,
length_scale = 2,
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] },
covariate_coeff = 0.05,
seed = 123
)
covariate_field = function(x) { x[1] + x[2] + 0.5 * x[3] }
sampled_points <- result$sampled_points
nodes <- as.matrix(read.table("/Users/Synne/Documents/Skole/H2024/master/code/Point_Processes/mesh_nodes.txt
"))
list.files("/Users/Synne/Documents/Skole/H2024/master/code/Point_Processes")
nodes <- as.matrix(read.table("mesh_nodes.txt
"))
nodes <- read.table("mesh_nodes.txt
")
nodes <- read.table("mesh_nodes.txt
")
